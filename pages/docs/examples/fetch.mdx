import { CodeGroup, CardGroup, Card, VersionBadge } from "src/shared/Docs/mdx";


export const description = "Make durable HTTP requests within an Inngest function.";

# Fetch: performing API requests or fetching data <VersionBadge version="TypeScript only" />

The Inngest TypeScript SDK provides a `step.fetch()` API and a `fetch()` utility, enabling you to make requests to third-party APIs or fetch data in a durable way by offloading them to the Inngest Platform.

For more information on how Fetch works, see the [Fetch documentation](/docs/features/inngest-functions/steps-workflows/fetch).

## Getting started with `step.fetch()`

The `step.fetch()` API enables you to make durable HTTP requests while offloading them to the Inngest Platform, saving you compute and improving reliability:

```ts {{ title: "src/inngest/functions.ts" }}
import { inngest } from "./client";

export const retrieveTextFile = inngest.createFunction(
  { id: "retrieveTextFile" },
  { event: "textFile/retrieve" },
  async ({ step }) => {
    // The fetching of the text file is offloaded to the Inngest Platform
    const response = await step.fetch(
      "https://example-files.online-convert.com/document/txt/example.txt"
    );

    // The Inngest function run is resumed when the HTTP request is complete
    await step.run("extract-text", async () => {
      const text = await response.text();
      const exampleOccurences = text.match(/example/g);
      return exampleOccurences?.length;
    });
  }
);
```

`step.fetch()` takes the same arguments as the [native `fetch` API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch).

<CardGroup cols={1}>

  <Card
    href={"https://github.com/inngest/inngest-js/tree/main/examples/node-step-fetch/"}
    title={"Clone this example on GitHub"}
    iconPlacement="top"
  >
    Check out this complete `step.fetch()` example on GitHub.
  </Card>

</CardGroup>


## Parallelize HTTP requests with `step.fetch()`

`step.fetch()` shares all the benefits of `step.run()`, including the ability to parallelize requests using `Promise.all()`:

```typescript
const processFiles = inngest.createFunction(
  { id: "process-files", concurrency: 10 },
  { event: "files/process" },
  async ({ step, event }) => {
    // All requests will be offloaded and processed in parallel while matching the concurrency limit
    const responses = await Promise.all(event.data.files.map(async (file) => {
      return step.fetch(`https://api.example.com/files/${file.id}`)
    }))

    // Your Inngest function is resumed here with the responses
    await step.run("process-file", async (file) => {
      const body = await response.json()
      // body.files
    })
  }
)
```
Note that `step.fetch()`, like all other `step` APIs, matches your function's configuration such as [concurrency](/docs/guides/concurrency) or [throttling](/docs/guides/throttling).

## Make 3rd party library HTTP requests durable with the `fetch()` utility

Inngest's `fetch()` utility can be passed as a custom fetch handler to make all the requests made by a 3rd party library durable.

For example, you can pass the `fetch()` utility to the AI SDK or the OpenAI libraries:

<CodeGroup>
```typescript {{ title: "AI SDK" }}
import { generateText } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

const weatherFunction = inngest.createFunction(
  { id: "weather-function" },
  { event: "weather/get" },
  async ({ step }) => {
    // This request is offloaded to the Inngest platform
    // and it also retries automatically if it fails!
    const response = await generateText({
      model: anthropic('claude-3-5-sonnet-20240620'),
      prompt: `What's the weather in London?`,
    });
  }
)
```
```typescript {{ title: "OpenAI SDK" }}
import { fetch } from "inngest";
import OpenAI from 'openai';

const client = new OpenAI({ fetch });

// use the global fetch
const completion = await client.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{ role: "user", content: "Hello, world!" }],
});

const weatherFunction = inngest.createFunction(
  { id: "weather-function" },
  { event: "weather/get" },
  async ({ step }) => {
    // The OpenAI request is automatically offloaded to the Inngest Platform
    const completion = await client.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [{ role: "user", content: "What's the weather in London?" }],
    });
  }
)
```
</CodeGroup>

## Using Inngest fetch with AI SDK: Best practices

When using Inngest's `fetch` (either `step.fetch` or the `fetch` utility) with the AI SDK, it's important to disable the AI SDK's built-in retry mechanism to avoid timeout issues and confusing error messages.

### Disable AI SDK retries

The AI SDK includes built-in retry logic that can conflict with Inngest's retry handling. This is particularly problematic when:
- Working with long-running models (e.g., OpenAI's o3)
- Deploying on serverless platforms with timeout limits (e.g., Vercel's 15-minute max)
- The combined retry duration exceeds your platform's timeout limit

**The problem:**
When both AI SDK and Inngest retry mechanisms are active, a timeout can occur where Vercel cancels the request, but Inngest shows it as an "Internal server error" (from Inngest, not Vercel). This makes debugging extremely difficult.

**The solution:**
Set `maxRetries: 0` in your AI SDK calls and let Inngest handle all retries:

<CodeGroup>
```typescript {{ title: "generateText() example" }}
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export const generateContent = inngest.createFunction(
  { id: "generate-content-o3" },
  { event: "content/generate" },
  async ({ event }) => {
    // Set maxRetries to 0 - let Inngest handle retries
    const response = await generateText({
      model: openai('o3'),
      prompt: event.data.prompt,
      maxRetries: 0, // Critical: Disable AI SDK retries
    });
    
    return response.text;
  },
);
```

```typescript {{ title: "streamText() example" }}
import { streamText } from 'ai';
import { anthropic } from '@ai-sdk/anthropic';

export const streamCompletion = inngest.createFunction(
  { id: "stream-completion" },
  { event: "completion/stream" },
  async ({ event }) => {
    // Set maxRetries to 0 for streaming operations too
    const result = await streamText({
      model: anthropic('claude-3-5-sonnet-20240620'),
      prompt: event.data.prompt,
      maxRetries: 0, // Let Inngest handle retries
    });
    
    // Process the stream
    for await (const chunk of result.textStream) {
      // Handle chunk
    }
    
    return result;
  },
);
```

```typescript {{ title: "Using step.fetch directly" }}
import { inngest } from './client';
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

export const analyzeDocument = inngest.createFunction(
  { id: "analyze-document" },
  { event: "document/analyze" },
  async ({ event, step }) => {

    const analysis = await generateText({
      model: openai('gpt-4'),
      prompt: `Analyze this document: ${event.data.content}`,
      maxRetries: 0, // Always disable AI SDK retries
    });
    
    return analysis.text;
  },
);
```
</CodeGroup>

**Benefits of this approach:**
- **Avoid timeout issues:** Prevent exceeding serverless platform timeout limits
- **Clearer error messages:** When failures occur, you'll see the actual error instead of a confusing "Internal server error"
- **Better observability:** Use Inngest's platform to see exactly what happened during each retry attempt
- **Optimized for long-running operations:** Inngest's retry mechanism is designed for durable, long-running tasks
