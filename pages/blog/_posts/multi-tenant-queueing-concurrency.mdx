---
heading: "Optimizing Queueing in Multi-Tenant Systems: Solving the Noisy Neighbor Problem"
subtitle: How to ensure fair processing in multi-tenant queueing systems
image: /assets/blog/soc2-compliant/featured-image.png
date: 2024-06-24
author: Dan Farrelly
disableCTA: true
---

Managing queues in multi-tenant systems can be challenging. Depending on the system, power users can monopolize resources, leading to a poor experience for other users. These "noisy neighbors" can cause delays and slow processing times leaving other users frustrated with a suboptimal experience.

In these systems it's important to ensure fairness and consistent performance for all users, regardless of their usage levels. This is difficult to do, but we'll break down this problem and potential solutions.

_If you're a visual learner, check out the video we recorded on this topic:_

<YouTube id="uoXqqMkuiP0" height={300}/>


## Noisy neighbors and queueing systems

In multi-tenant systems, users share the same infrastructure resources. A "[noisy neighbor](https://learn.microsoft.com/en-us/azure/architecture/antipatterns/noisy-neighbor/noisy-neighbor)" refers to a high-usage user consuming excessive resources, thereby affecting other users' performance. This can lead to delays and frustration for users with lower usage levels.

Here is a visualization of a single user monopolizing a first-in-first-out (FIFO) queue:

<video controls autoPlay loop muted src="/assets/blog/multi-tenant-queueing-concurrency/noisy-neighbor.mp4" />

As we can see in this visualization, the high-usage user is causing delays for other users.

## Ensuring fairness in multi-tenant systems

The goal in building multi-tenant systems is to provide a fair and consistent experience for all users. Without considering fairness, users may confuse slow processing times with poor system performance.

Here is a visualization of a fair queueing system where each user receives equal processing time:

<video controls autoPlay loop muted src="/assets/blog/multi-tenant-queueing-concurrency/fairness-queue.mp4" />

### Achieving fairness - A queue per user

An ideal approach to fairness is to provide a queue per user. This way, each user's workloads are isolated, preventing any single user from affecting others. However, this approach introduces several challenges:

* **Infrastructure management**: Creating and maintaining thousands of individual queues is cumbersome and complex. This also might represent a shift towards a single-tenant architecture which may not be desirable.
* **Smart workers**: Workers need to be designed to scan and pull from multiple queues intelligently to prioritize work effectively. It's not likely that you can afford a worker-per user at scale so workers need to be able to handle multiple queues and re-adjust as more users are added to your system.
* **Defining queues ahead of time**: Most traditional message queues require pre-configuring your queues either in infrastructure-as-code or programmatically. This may work for a small number of users, but it's not practical at scale. Additionally, you'll need some sort of state management to keep track of all these queues.

<img src="/assets/blog/multi-tenant-queueing-concurrency/queue-per-user.png" alt="A visualization of a single queue fanning out into a queue per user" />

### Alternative approaches - Priority queues

An alternative approach might be to use low/high priority queues, but to utilize this approach to to solve noisy neighbor problems, the system would have to track the high-usage users and redistribute workloads to low priority queues.

This is similar to API request throttling where a user's rate of requests is tracked and if they exceed a certain threshold, they are pushed into a lower priority queue.

<img src="/assets/blog/multi-tenant-queueing-concurrency/priority-queue.png" alt="A visualization of a priority queue that first determines the rate of jobs" />

## Inngest: Concurrency limits for each tenant

The Inngest platform combines event streams, queues, and durable execution into a single system. A core component of the system is built-in flow control which allows you to configure how and when your functions (_jobs_) are executed.

In comparison with the previously mentioned methods which require a combination of queueing infrastructure and worker logic, Inngest manages all infrastructure and has embedded flow control logic within the queue itself. This removes the need for custom worker logic and state management. This means:

* You define the concurrency `limit` in your function configuration.
* Specifying a concurrency `key` will apply this limit to _each unique value of the key_. This key can be anything relevant to your system, such as user ID, organization ID, email, workspace ID, etc.
* As new runs (aka _jobs_) are created, **Inngest dynamically creates a new queue for each unique key value**. The system internally handles the complexity of managing these queues, so you don't have to.

Let's take a look at some example code:

```ts multiTenantConcurrency.ts
export const multiTenantConcurrency = inngest.createFunction(
  {
    id: 'multi-tenant-concurrency',
    concurrency: [
      {
        limit: 1,
        // !mark(1:2)
        // Add a key to apply the concurrency limit to each unique user_slug
        key: 'event.data.user_slug',
      },
    ],
  },
  { event: 'demo/job.created' },
  async ({ event, step }) => {
    /* function logic omitted for example */
    return { status: 'success' };
  }
);
```

### Going further - Combining concurrency limits

Inngest allows you to "stack" or combine concurrency limits. This means you can limit the concurrent items for individual users while also setting a total concurrency limit for _all_ users. This provides a flexible way to manage resources. Let's take a look at a function that limits the concurrency to `5` per user, but also limits the total concurrency to `100`:

```ts combinedConcurrencyLimits.ts
export const combinedConcurrencyLimits = inngest.createFunction(
  {
    id: 'multi-tenant-concurrency-with-shared-limit',
    concurrency: [
      {
        // !mark(1:3)
        // Each unique user_slug has a concurrency limit of 5
        limit: 5,
        key: 'event.data.user_slug',
      },
      {
        // !mark(1:2)
        // Overall concurrency for all users is limited to 100
        limit: 100,
      },
    ],
  },
  { event: 'demo/job.created' },
  async ({ event, step }) => {
    /* function logic omitted for example */
    return { status: 'success' };
  }
);
```

### Further Flexibility

Beyond using keys and combining concurrency limits, for additional control within your system, you might also consider:

* **Setting concurrency limits that are shared across functions** - Concurrency limits can be shared across multiple functions by [specifying a `scope`](/docs/guides/concurrency#how-global-limits-work) which allows to combine limits across functions.
* **Combining with other flow control methods** - Inngest's concurrency limits can be combined with other methods like [throttling](/docs/guides/throttling) and [prioritization](/docs/guides/priority), giving you even more control over your system's workload management.

## Give it a try

Managing queues in multi-tenant systems can be complex, but with Inngest's built-in multi-tenant-aware concurrency controls, you can ensure a fair, efficient, and smooth experience in your applications. Learn more about how to implement concurrency limits in your functions by checking out the [Inngest Concurrency Documentation](/docs/guides/concurrency).