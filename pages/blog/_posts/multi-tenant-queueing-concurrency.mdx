---
heading: "Optimizing Queueing in Multi-Tenant Systems: Solving the Noisy Neighbor Problem"
subtitle: How to ensure fair processing in multi-tenant queueing systems
image: /assets/blog/soc2-compliant/featured-image.png
date: 2024-06-24
author: Dan Farrelly
disableCTA: true
---

Managing queues in multi-tenant systems can be challenging. Depending on the system, power users can monopolize resources, leading to a poor experience for other users. These "noisy neighbors" can cause delays and slow processing times leaving other users frustrated with a suboptimal experience.

In these systems it's important to ensure fairness and consistent performance for all users, regardless of their usage levels. This is difficult to do, but we'll break down this problem and potential solutions.

_If you're a visual learner, check out the video we recorded on this topic:_

<YouTube id="uoXqqMkuiP0" height={300}/>


## Noisy neighbors and queueing systems

In multi-tenant systems, users share the same infrastructure resources. A "[noisy neighbor](https://learn.microsoft.com/en-us/azure/architecture/antipatterns/noisy-neighbor/noisy-neighbor)" refers to a high-usage user consuming excessive resources, thereby affecting other users' performance. This can lead to delays and frustration for users with lower usage levels.

Here is a visualization of a single user monopolizing a first-in-first-out (FIFO) queue:

<video controls autoPlay loop muted src="/assets/blog/multi-tenant-queueing-concurrency/noisy-neighbor.mp4" />

As we can see in this visualization, the high-usage user is causing delays for other users.

## Ensuring fairness in multi-tenant systems

The goal in building multi-tenant systems is to provide a fair and consistent experience for all users. Without considering fairness, users may confuse slow processing times with poor system performance.

Here is a visualization of a fair queueing system where each user receives equal processing time:

<video controls autoPlay loop muted src="/assets/blog/multi-tenant-queueing-concurrency/fairness-queue.mp4" />

### Achieving fairness - A queue per user

An ideal approach to fairness is to provide a queue per user. This way, each user's workloads are isolated, preventing any single user from affecting others. However, this approach introduces several challenges:

* **Infrastructure management**: Creating and maintaining thousands of individual queues is cumbersome and complex. This also might represent a shift towards a single-tenant architecture which may not be desirable.
* **Smart workers**: Workers need to be designed to scan and pull from multiple queues intelligently to prioritize work effectively. It's not likely that you can afford a worker-per user at scale so workers need to be able to handle multiple queues and re-adjust as more users are added to your system.
* **Defining queues ahead of time**: Most traditional message queues require pre-configuring your queues either in infrastructure-as-code or programmatically. This may work for a small number of users, but it's not practical at scale. Additionally, you'll need some sort of state management to keep track of all these queues.

<img src="/assets/blog/multi-tenant-queueing-concurrency/queue-per-user.png" alt="A visualization of a single queue fanning out into a queue per user" />

### Alternative approaches - Priority queues

An alternative approach might be to use low/high priority queues, but to utilize this approach to to solve noisy neighbor problems, the system would have to track the high-usage users and redistribute workloads to low priority queues.

This is similar to API request throttling where a user's rate of requests is tracked and if they exceed a certain threshold, they are pushed into a lower priority queue.

<img src="/assets/blog/multi-tenant-queueing-concurrency/priority-queue.png" alt="A visualization of a priority queue that first determines the rate of jobs" />

## Inngest: Built-In Support for Concurrency Limits

Inngest offers a robust solution to this problem with built-in support for concurrency limits:

Concurrency Limit with "Key" Property: You can specify a concurrency limit using a "key" property, which applies this limit to each unique value of the key. This key can be anything relevant to your system, such as user ID, organization ID, email, workspace ID, etc.

```ts function.ts
export const multiTenantConcurrency = inngest.createFunction(
  {
    id: 'multi-tenant-concurrency',
    concurrency: [
      {
        limit: 1,
        // !mark(1:2)
        // Add a key to apply the concurrency limit to each unique user_slug
        key: 'event.data.user_slug',
      },
    ],
  },
  { event: 'demo/job.created' },
  async ({ event, step }) => {
    /* function logic omitted for example */
    return { status: 'success' };
  }
);
```

Automatic Queue Management: Inngest dynamically creates a new queue for each unique key value. The system's workers handle the complexity of managing these queues, so you don't have to.

Function-Specific Controls: You can apply these concurrency controls uniquely for each function (job) in your system.

What It Looks Like After Applying Inngest
After implementing Inngest's concurrency limits, the system might look like this:

 (Note: Replace with actual GIF/video showing the solution)

This visualization shows a balanced system where no single user can monopolize the queue, ensuring a fair and efficient processing environment.

Additional Features and Flexibility
Stacking Concurrency Limits
Inngest allows you to stack or combine concurrency limits. This means you can limit the concurrent items for individual users while also setting a total concurrency limit for all users. This provides a flexible way to manage resources.

Shared Across Functions
Concurrency limits can be shared across multiple functions by specifying a scope, allowing for consistent control across your system.

Combining with Other Flow Control Methods
Inngest's concurrency control can be combined with other methods like throttling and prioritization, giving you comprehensive control over your system's workload management.

Give It a Try
Ready to optimize your multi-tenant system's queue management? Check out the Inngest Concurrency Documentation to get started.

By leveraging Inngest's built-in support for concurrency limits, you can ensure a fair, efficient, and user-friendly experience in your multi-tenant applications. Say goodbye to the noisy neighbor problem and hello to seamless queue management.


- problem: multi tenant systems with queues
  - noisy neighbors - high usage users can block other users
  - bad experience for other users - slow processing, bad UX
  - fairness
- This is what it looks like before - a GIF/video of the problem
- this visualization is simplified, but shows the concept
- How do you fix this?
  - Ideally, you'd have a queue per user
  - this is hard b/c with more traditional queues, you need to define these queues ahead of time - in the infrastructure, or maybe in the code ahead of time.
  - You don't want to manage 1000s of queues
  - you then need to have workers that can pull from these queues and be smart by checking multiple queues
  - some systems might
  - you might also set up low/high priority queues - but you need to be able to track the rate of each user and push high volume users to a different queue. manual tedious
- Inngest has this support built in
  - you specify a concurrency limit and use a "key" property
  - this key applies this limit to each unique value of this key
  - it can be anything - user id, organization id, email, workspace id, etc.
  - Inngest has built support for this directly into our queue
  - Behind the scenes, we spin up a new queue for each unique value of this key and our own workers handle this complexity for you
  - You can specify this control uniquely for each function (aka job) in your system
- what this looks like after this is applied: a GIF/video of the solution
- what else you can do:
  - you can "stack" or combine concurrency limits so you can limit the concurrent items for a user, but also limit the total concurrency for all users
  - this can also be shared across functions by specifying a scope
  - you can also combine with other methods of flow control like throttling, prioritization, etc.
- give it a try - check out the concurrency docs here